# ğŸ§  BASS++ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„é«˜ååæ¨æµ‹æ¨ç†ç®—æ³•ä¸ç³»ç»Ÿä¼˜åŒ–

*(Batched Attention-optimized Speculative Sampling for Efficient Large Language Model Inference)*

------

## ä¸€ã€é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®åŸºäº **æ¨æµ‹æ¨ç†ï¼ˆSpeculative Decodingï¼‰** æŠ€æœ¯ï¼Œæå‡ºå¹¶å®ç°äº† **BASS++ï¼ˆBatched Attention-optimized Speculative Samplingï¼‰** æ¡†æ¶ï¼Œ
 ç”¨äºåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„å‰æä¸‹æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†é€Ÿåº¦ä¸å¹¶è¡Œååæ€§èƒ½ã€‚

æœ¬ä»“åº“åŒ…å«ï¼š

- **ä»»åŠ¡ä¸€**ï¼šæ¨æµ‹æ¨ç†ä¸æ‰¹é‡åŠ é€Ÿç®—æ³•å®ç°ï¼ˆSpeculative / BASSï¼‰ï¼›
- **ä»»åŠ¡äºŒ**ï¼šåŸºäº Dolly æ•°æ®é›†ä¸è‡ªå®šä¹‰æ‰¹æ¬¡çš„æ€§èƒ½è¯„æµ‹ä¸åˆ†æã€‚

------

## äºŒã€é¡¹ç›®ç»“æ„

```
LLMSpeculativeSampling/
â”œâ”€â”€ main.py                  # ä¸»æ¨ç†è„šæœ¬ï¼ˆä»»åŠ¡ä¸€ï¼‰
â”œâ”€â”€ eval_dolly.py            # åŸºäº Dolly æ•°æ®é›†çš„è¯„æµ‹è„šæœ¬ï¼ˆä»»åŠ¡äºŒï¼‰
â”œâ”€â”€ serving.py               # Flask Web æœåŠ¡æ¥å£ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ requirements.txt         # ç¯å¢ƒä¾èµ–
â”‚
â”œâ”€â”€ benchmark/               # ä»»åŠ¡äºŒæ‰¹é‡æµ‹è¯•ä¸åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ analyzer.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â”œâ”€â”€ runner.py
â”‚   â”œâ”€â”€ runner_support.py
â”‚   â””â”€â”€ utils_io.py
â”‚
â”œâ”€â”€ sampling/                # æ ¸å¿ƒç®—æ³•æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ autoregressive_sampling.py
â”‚   â”œâ”€â”€ speculative_sampling.py
â”‚   â”œâ”€â”€ speculative_bass.py
â”‚   â”œâ”€â”€ kvcache_model.py
â”‚   â”œâ”€â”€ batched_kvcache_model.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ utils_bass.py
â”‚   â””â”€â”€ globals.py
â”‚
â”œâ”€â”€ results/                 # å®éªŒè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¿å­˜åœ¨æœ¬åœ°æ ¹ç›®å½•ï¼‰
â””â”€â”€ LICENSE.txt
```

------

## ä¸‰ã€æ•°æ®ä¸æ¨¡å‹ç›®å½•è¯´æ˜

> âš ï¸ æ³¨æ„ï¼š`data/` ç›®å½•ä»…å­˜åœ¨äºæœåŠ¡å™¨ç«¯ï¼Œæœ¬åœ°å¼€å‘ç¯å¢ƒä¸­æ— éœ€åŒæ­¥ã€‚

æœåŠ¡å™¨ç«¯ç›®å½•ç»“æ„ç¤ºä¾‹ï¼š

```
data/
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ dolly/
â”‚       â””â”€â”€ databricks-dolly-15k.jsonl
â””â”€â”€ models/
    â”œâ”€â”€ bloom-560m/
    â”œâ”€â”€ bloomz-7b1/
    â”œâ”€â”€ TinyLlama-1B/
    â””â”€â”€ Llama-2-7b-raw/
```

æœ¬åœ°è¿è¡Œè„šæœ¬æ—¶é€šè¿‡ç›¸å¯¹è·¯å¾„ `../data/models/...` å¼•ç”¨ã€‚

------

## å››ã€ç®—æ³•åŸç†ç®€è¿°

### ğŸ”¹ æ¨æµ‹æ¨ç†ï¼ˆSpeculative Decodingï¼‰

- å°æ¨¡å‹è‰ç¨¿é¢„æµ‹ Î³ ä¸ªå€™é€‰ tokenï¼›
- å¤§æ¨¡å‹éªŒè¯æ¥å—å‰ç¼€ï¼›
- ä¸€è‡´éƒ¨åˆ†æ‰¹é‡æ¥å—ï¼Œä»è€Œå‡å°‘ç›®æ ‡æ¨¡å‹å‰å‘è°ƒç”¨ã€‚

### ğŸ”¹ BASS-PADï¼ˆæ‰¹é‡æ¨ç†ï¼‰

- å¤šåºåˆ—å¹¶è¡Œè‰ç¨¿éªŒè¯ï¼›
- ä½¿ç”¨ PAD å¯¹é½åºåˆ—ï¼›
- KV Cache æ‰¹é‡ç®¡ç†ä¸å›æ»šï¼›
- å…¼å®¹ LLaMAã€BLOOMã€OPT ç­‰ä¸»æµæ¶æ„ã€‚

### ğŸ”¹ åŠ¨æ€è‰ç¨¿é•¿åº¦ Î³ è°ƒåº¦

æ ¹æ®æ¥å—ç‡ Î± è‡ªé€‚åº”æ›´æ–°è‰ç¨¿é•¿åº¦ï¼š
 [
 \gamma_{t+1} =
 \begin{cases}
 \min(\gamma_t + \Delta_1, \gamma_{\max}), & \text{è‹¥å…¨éƒ¨æ¥å—}[4pt]
 \max(1, \max(x_i), \gamma_t - \lceil \gamma_t/m \rceil - s), & \text{å¦åˆ™}
 \end{cases}
 ]

------

## äº”ã€è¿è¡Œç¯å¢ƒé…ç½®

### 1ï¸âƒ£ åˆ›å»ºç¯å¢ƒ

```bash
conda create -n bass python=3.10
conda activate bass
pip install -r requirements.txt
```

### 2ï¸âƒ£ æ¨èç¯å¢ƒå˜é‡

åœ¨ PyCharm æˆ–å‘½ä»¤è¡Œè¿è¡Œæ—¶å»ºè®®è®¾ç½®ï¼š

```bash
PYTHONUNBUFFERED=1
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64
```

------

## å…­ã€è„šæœ¬ä½¿ç”¨æ–¹æ³•

### ğŸ”¹ 1. main.py â€”â€” å•æ¡æˆ–æ‰¹é‡æ¨ç†ï¼ˆä»»åŠ¡ä¸€ï¼‰

å‘½ä»¤è¡Œç¤ºä¾‹ï¼ˆBLOOM æ¨¡å‹ï¼‰ï¼š

```bash
python main.py \
  --input "The quick brown fox jumps over the lazy " \
  --target_model_name ../data/models/bloomz-7b1 \
  --approx_model_name ../data/models/bloom-560m \
  -v --bass --batch 32
```

ä¹Ÿå¯å°†æ¨¡å‹æ›¿æ¢ä¸º LLaMAï¼š

```bash
python main.py \
  --input "äººå·¥æ™ºèƒ½çš„æ¨ç†æœºåˆ¶åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ" \
  --target_model_name ../data/models/Llama-2-7b-raw \
  --approx_model_name ../data/models/TinyLlama-1B \
  --bass --batch 16
```

**å‚æ•°è¯´æ˜ï¼š**

| å‚æ•°                  | ç±»å‹ | è¯´æ˜                       |
| --------------------- | ---- | -------------------------- |
| `--input`             | str  | è¾“å…¥æ–‡æœ¬å‰ç¼€               |
| `--target_model_name` | path | ç›®æ ‡æ¨¡å‹è·¯å¾„               |
| `--approx_model_name` | path | è‰ç¨¿æ¨¡å‹è·¯å¾„               |
| `--batch`             | int  | æ‰¹å¤§å°                     |
| `--bass`              | flag | å¯ç”¨ BASS-PAD æ‰¹é‡æ¨ç†æ¨¡å¼ |
| `-v`                  | flag | è¾“å‡ºè¯¦ç»†æ—¥å¿—               |

------

### ğŸ”¹ 2. eval_dolly.py â€”â€” Dolly æ•°æ®é›†æ‰¹é‡è¯„æµ‹ï¼ˆä»»åŠ¡äºŒï¼‰

å‘½ä»¤ç¤ºä¾‹ï¼š

```bash
python eval_dolly.py \
  --target_model_name ../data/models/Llama-2-7b-raw \
  --approx_model_name ../data/models/TinyLlama-1B \
  --batch 32 \
  --samples 256 \
  --max_tokens 20 \
  --gamma_init 7 \
  --temperature 0.9 \
  --top_p 0.9 \
  --top_k 100 \
  --modes deepmind,google,BASS \
  --dataset_path ../data/dataset/dolly/databricks-dolly-15k.jsonl
```

è¯¥è„šæœ¬ä¼šè¿è¡Œä¸‰ç§æ¨¡å¼ï¼ˆDeepMind / Google / BASSï¼‰ï¼Œåœ¨ Dolly æ•°æ®é›†ä¸Šè¯„ä¼°æ¨ç†æ—¶é—´ä¸ç”Ÿæˆç»“æœä¸€è‡´æ€§ã€‚

> âš ï¸ æ³¨æ„ï¼šæœ¬è„šæœ¬ **ä¸ä¼šè‡ªåŠ¨å†™å…¥ results ç›®å½•**ï¼Œè€Œæ˜¯ç›´æ¥åœ¨æ§åˆ¶å°è¾“å‡ºæ€§èƒ½ç»Ÿè®¡ã€‚

**ä¸»è¦å‚æ•°ï¼š**

| å‚æ•°                  | ç±»å‹  | è¯´æ˜                   |
| --------------------- | ----- | ---------------------- |
| `--target_model_name` | path  | ç›®æ ‡æ¨¡å‹è·¯å¾„           |
| `--approx_model_name` | path  | è‰ç¨¿æ¨¡å‹è·¯å¾„           |
| `--batch`             | int   | æ‰¹å¤§å°                 |
| `--samples`           | int   | é‡‡æ ·æ ·æœ¬æ•°             |
| `--max_tokens`        | int   | æ¯æ¡ç”Ÿæˆæœ€å¤§ token æ•°  |
| `--gamma_init`        | int   | è‰ç¨¿åˆå§‹æ­¥é•¿           |
| `--temperature`       | float | softmax æ¸©åº¦           |
| `--top_p`             | float | nucleus sampling å‚æ•°  |
| `--top_k`             | int   | top-k é‡‡æ ·å‚æ•°         |
| `--modes`             | list  | æŒ‡å®šå®éªŒæ¨¡å¼ï¼ˆå¯å¤šé€‰ï¼‰ |
| `--dataset_path`      | path  | Dolly æ•°æ®é›†è·¯å¾„       |

------

### ğŸ”¹ 3. benchmark.run_experiment â€”â€” æ¨¡å—åŒ–ä»»åŠ¡äºŒæ€§èƒ½åˆ†æ

`benchmark/run_experiment.py` ä¸ºæ¨¡å—æ‰§è¡Œè„šæœ¬ï¼Œæ¨èä»¥ *module* æ–¹å¼è¿è¡Œï¼ˆPyCharm æˆ–å‘½ä»¤è¡Œå‡å¯ï¼‰ï¼š

å‘½ä»¤è¡Œç¤ºä¾‹ï¼š

```bash
python -m benchmark.run_experiment
```

æˆ–åœ¨ PyCharm â€œRun Configurationâ€ ä¸­é…ç½®ï¼š

- **Module name:** `benchmark.run_experiment`
- **Working directory:** é¡¹ç›®æ ¹ç›®å½• `LLMSpeculativeSampling`
- **Environment variables:**
   `PYTHONUNBUFFERED=1;PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64`

è¯¥æ¨¡å—è´Ÿè´£ä»»åŠ¡äºŒçš„å®Œæ•´æ€§èƒ½æµ‹è¯•æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

- åŠ è½½æ¨¡å‹ç»„åˆï¼›
- è‡ªåŠ¨æ‰¹é‡æ¨ç†ï¼›
- è®°å½•å¹³å‡ååä¸å»¶è¿Ÿï¼›
- ç”Ÿæˆ CSV ä¸å›¾è¡¨è¾“å‡ºï¼ˆå­˜æ”¾åœ¨ `results/` ä¸‹ï¼‰ã€‚

------

## ä¸ƒã€å®éªŒç»“æœç¤ºä¾‹

| æ¨¡å‹ç»„åˆ                | æ¨¡å¼                                | ååæå‡ | å»¶è¿Ÿé™ä½  | GPU åˆ©ç”¨ç‡ |
| ----------------------- | ----------------------------------- | -------- | --------- | ---------- |
| Bloom-560M + Bloomz-7B1 | Baseline                            | 1Ã—       | 1Ã—        | 4%         |
| â†‘                       | Speculative Decoding                | 2.1Ã—     | 0.65Ã—     | 8%         |
| â†‘                       | BASS-PAD                            | **2.8Ã—** | **0.55Ã—** | **12%**    |
| â†‘                       | BASS++ (Î³ è‡ªé€‚åº” + Sorted Batching) | **3.1Ã—** | **0.45Ã—** | **15%+**   |

------

## å…«ã€è®¸å¯è¯ä¸å¼•ç”¨

æœ¬é¡¹ç›®åŸºäº [FeiFeiBear/speculative-decoding-demo](https://github.com/FeiFeiBear/speculative-decoding-demo)
 å¼€æºå®ç°è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œéµå¾ª **Apache License 2.0**ã€‚

å¼•ç”¨è®ºæ–‡ï¼š

- Leviathan et al., *Fast Inference from Transformers via Speculative Decoding*, ICML 2023.
- Chen et al., *Accelerating Large Language Model Decoding with Speculative Sampling*, arXiv 2023.
- AWS AI Lab, *BASS: Speculative Sampling for Batch Decoding*, 2024.

------

## ä¹ã€è”ç³»æ–¹å¼

- å›¢é˜Ÿåç§°ï¼š**DeepSpec æ·±æ¨å›¢é˜Ÿ**
- è”ç³»äººï¼šXXX
- é‚®ç®±ï¼š[xxx@xxx.edu.cn](mailto:xxx@xxx.edu.cn)
- æ—¥æœŸï¼š2025 å¹´ 10 æœˆ

------

